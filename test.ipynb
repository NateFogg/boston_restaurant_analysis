{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from flatten_json import flatten\n",
    "\n",
    "from config import TOKEN\n",
    "\n",
    "url = 'https://api.yelp.com/v3/businesses/search?location=Allston%2C%20Boston&term=restaurants&sort_by=distance&limit=50&offset=0'\n",
    "\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Authorization': 'Bearer ' + TOKEN\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    json_data = response.json()\n",
    "else:\n",
    "    # Handle the API request error here\n",
    "    print('Failed to retrieve data from the API.')\n",
    "    json_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if json_data:\n",
    "    businesses = pd.DataFrame()\n",
    "\n",
    "    for dict in json_data['businesses']:\n",
    "        # flatten the data\n",
    "        bus_row = pd.json_normalize(dict) \n",
    "        \n",
    "        # append the new row to data frame\n",
    "        businesses = pd.concat([businesses, bus_row])\n",
    "\n",
    "    print(businesses.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first need to join all the data from neighborhood sampling so we can perform the code below (keep unique, calculate how many overlap)\n",
    "\n",
    "# make a new row for each dictionary in the categories col\n",
    "bus_exploded = businesses.explode('categories').reset_index(drop=True)\n",
    "\n",
    "# encode all the information into new binary categorical columns \n",
    "bus_encoded = pd.get_dummies(bus_exploded['categories'].apply(pd.Series))\n",
    "\n",
    "# concat the new columns to the exploded dataframe so that the rows match\n",
    "bus_final = pd.concat([bus_exploded, bus_encoded], axis=1)\n",
    "\n",
    "# drop the titles\n",
    "bus_final = bus_final.loc[:,~bus_final.columns.str.startswith('title')]\n",
    "\n",
    "# need to make the rows unique and get the sum of alias cols by partitioning by business id \n",
    "grouped = bus_final.groupby('id')\n",
    "\n",
    "# Use 'transform' to calculate the sum of 'value1' and 'value2' within each group\n",
    "for col in bus_final.columns[24:]:\n",
    "    bus_final[col[6:]] = grouped[col].transform('sum')\n",
    "\n",
    "bus_final = bus_final.loc[:,~bus_final.columns.str.startswith('alias')]\n",
    "bus_final = bus_final.drop(columns=['categories', 'location.display_address'])\n",
    "\n",
    "bus_final = bus_final.drop_duplicates(subset = ['id'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# make a new row for each dictionary in the transaction col\n",
    "bus_exploded = bus_final.explode('transactions').reset_index(drop=True)\n",
    "\n",
    "# encode all the information into new binary categorical columns \n",
    "bus_encoded = pd.get_dummies(bus_exploded['transactions'].apply(pd.Series))\n",
    "\n",
    "# concat the new columns to the exploded dataframe so that the rows match\n",
    "bus_final = pd.concat([bus_exploded, bus_encoded], axis=1)\n",
    "\n",
    "grouped = bus_final.groupby('id')\n",
    "\n",
    "# Use 'transform' to calculate the sum of 'value1' and 'value2' within each group\n",
    "for col in bus_final.columns[-3:]:\n",
    "    bus_final[col[2:]] = grouped[col].transform('sum')\n",
    "\n",
    "bus_final = bus_final.drop(columns=['transactions'])\n",
    "bus_final = bus_final.loc[:,~bus_final.columns.str.startswith('0_')]\n",
    "bus_final = bus_final.drop_duplicates(subset = ['id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# feature engineering\n",
    "# has image from image_url\n",
    "# ranking that is a average of rating and review_count\n",
    "# has_phone from phone\n",
    "# maybe keep distance to verify the sampling\n",
    "# has_st_add from location.address1\n",
    "# verify the location is within Boston\n",
    "# has_price from price\n",
    "# encode the price options to different cols\n",
    "# change the names of the location cols\n",
    "bus_final['has_image'] = np.where(bus_final['image_url'] != '', 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "# cols to drop\n",
    "# image url \n",
    "# is_closed\n",
    "# url\n",
    "# phone\n",
    "# display_phone\n",
    "# location.address1\n",
    "# location.address2\n",
    "# location.address3\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (v3.9.12:b28265d7e6, Mar 23 2022, 18:17:11) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
