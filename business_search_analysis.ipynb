{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run business_search_data_load.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if valid_data:\n",
    "    restaurants = pd.DataFrame()\n",
    "    for rest in valid_data:\n",
    "        # flatten and append the new row to data frame\n",
    "        restaurants = pd.concat([restaurants, pd.json_normalize(rest)])\n",
    "\n",
    "    # drop duplicate restaurants\n",
    "    restaurants = restaurants.drop_duplicates(subset = ['id'])\n",
    "\n",
    "    # make a new row for each dictionary in the categories column\n",
    "    rest_exp = restaurants.explode('categories').reset_index(drop=True)\n",
    "\n",
    "    # encode all the information into new binary categorical columns \n",
    "    rest_enc = pd.get_dummies(rest_exp['categories'].apply(pd.Series))\n",
    "\n",
    "    # concat the new columns to the exploded dataframe so that the rows match\n",
    "    rest_final = pd.concat([rest_exp, rest_enc], axis=1)\n",
    "\n",
    "    # change all column names to string and drop titles\n",
    "    rest_final.columns = rest_final.columns.map(str)\n",
    "    rest_final = rest_final.loc[:,~rest_final.columns.str.startswith('title')]\n",
    "\n",
    "    # make rows unique and get the sum of categorical encoded cols grouping by business id \n",
    "    grouped = rest_final.groupby('id')\n",
    "    for col in rest_final.columns[25:]:\n",
    "        # col[:6] just drops the alias_ from the category name\n",
    "        rest_final[col[6:]] = grouped[col].transform('sum')\n",
    "\n",
    "    rest_final = rest_final.drop_duplicates(subset = ['id'])\n",
    "\n",
    "\n",
    "    # repeat above steps to make a new row for each dictionary in the transaction col\n",
    "    rest_exp = rest_final.explode('transactions').reset_index(drop=True)\n",
    "\n",
    "    # encode all the information into new binary categorical columns \n",
    "    rest_enc = pd.get_dummies(rest_exp['transactions'].apply(pd.Series))\n",
    "\n",
    "    # concat the new columns to the exploded dataframe so that the rows match\n",
    "    rest_final = pd.concat([rest_exp, rest_enc], axis=1)\n",
    "\n",
    "    grouped = rest_final.groupby('id')\n",
    "\n",
    "    # make rows unique and get the sum of categorical encoded cols grouping by business id \n",
    "    for col in rest_final.columns[-3:]:\n",
    "        rest_final[col[2:]] = grouped[col].transform('sum')\n",
    "\n",
    "    rest_final = rest_final.drop_duplicates(subset = ['id'])\n",
    "\n",
    "\n",
    "    # clean up\n",
    "    rest_final = rest_final.loc[:,~rest_final.columns.str.startswith('alias')]\n",
    "    rest_final = rest_final.loc[:,~rest_final.columns.str.startswith('0_')]\n",
    "    rest_final = rest_final[rest_final['review_count'] > 0]\n",
    "    rest_final = rest_final.drop(columns=['categories', 'location.state', 'location.country', 'location.display_address', 'transactions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "if not rest_final.empty:\n",
    "\n",
    "    # encode the neighborhoods\n",
    "    rest_final = pd.concat([rest_final, pd.get_dummies(rest_final['neighborhood'])], axis=1)\n",
    "\n",
    "    # make sure column names are strings\n",
    "    rest_final.columns = rest_final.columns.map(str)\n",
    "\n",
    "    # change all empty values to nan\n",
    "    rest_final = rest_final.replace('', np.nan)\n",
    "\n",
    "    # encode the price options to scale\n",
    "    rest_final['price'].replace({'$':1, '$$':2, '$$$':3, '$$$$':4}, inplace=True)\n",
    "    rest_final['price'].fillna(0, inplace=True)\n",
    "\n",
    "    # has image from image_url\n",
    "    rest_final['has_image'] = np.where(rest_final['image_url'].isna(), 0, 1)\n",
    "\n",
    "    # has_phone from phone\n",
    "    rest_final['has_phone'] = np.where(rest_final['phone'].isna(), 0, 1) \n",
    "\n",
    "    # has_st_add from location.address1\n",
    "    rest_final['has_st_add'] = np.where(rest_final['location.address1'].isna(), 0, 1) \n",
    "\n",
    "    # for regression feature importance calculate a Balanced Rating Score (BRS)\n",
    "    weight_average_rating = 0.7\n",
    "    weight_review_count = 0.3\n",
    "\n",
    "    # normalize review count using logarithm and min-max scaling\n",
    "    rest_final['norm_count'] = np.log10(rest_final['review_count'] + 0.000000001)\n",
    "    rest_final['norm_count'] = (rest_final['norm_count'] - rest_final['norm_count'].min()) / (rest_final['norm_count'].max() - rest_final['norm_count'].min())\n",
    "\n",
    "    # calculate brs\n",
    "    rest_final['brs'] = (weight_average_rating * (rest_final['rating'] / 5)) + (weight_review_count * rest_final['norm_count'])\n",
    "\n",
    "    # cols to drop and rename\n",
    "    rest_final = rest_final.drop(columns=['location.city', 'location.zip_code', 'image_url', 'is_closed', 'url', 'distance', 'norm_count', 'phone', 'display_phone', 'location.address1', 'location.address2', 'location.address3'])\n",
    "    rest_final = rest_final.rename(columns={'coordinates.latitude': 'latitude', 'coordinates.longitude': 'longitude'})\n",
    "    \n",
    "    # check the integrity of the data\n",
    "    print(f'Missing Values Detected: {rest_final.isnull().values.any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import geoplot\n",
    "import geoplot.crs as gcrs\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# create kde plot of restaurant location density\n",
    "bos_map = gpd.read_file('Boston_Neighborhoods/Boston_Neighborhoods.shp')\n",
    "geometry = [Point(xy) for xy in zip(rest_final['longitude'], rest_final['latitude'])]\n",
    "crs = {'init':'epsg:4326'}\n",
    "geo_df = gpd.GeoDataFrame(rest_final, # specify our data\n",
    "                          crs=crs, # specify our coordinate reference system\n",
    "                          geometry=geometry) # specify the geometry list we created\n",
    "ax = geoplot.polyplot(bos_map, projection=gcrs.AlbersEqualArea(), zorder=2, figsize=(15, 15))\n",
    "geoplot.kdeplot(geo_df, cmap='Reds', thresh=0, fill=True, clip=bos_map, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# distribution of balanced rating score\n",
    "custom_bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "plt.hist(rest_final['brs'], bins=custom_bins, color='r', edgecolor='black', alpha=0.7)\n",
    "median = rest_final['brs'].median()\n",
    "mean = rest_final['brs'].mean()\n",
    "plt.axvline(median, color='k', linestyle='dashed', linewidth=1, label='Median BRS')\n",
    "plt.axvline(mean, color='k', linestyle='solid', linewidth=1, label='Mean BRS')\n",
    "plt.xlabel('BRS')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of BRS')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# distribution of review counts\n",
    "plt.hist(rest_final['review_count'], color='b', edgecolor='black', alpha=0.7)\n",
    "median = rest_final['review_count'].median()\n",
    "mean = rest_final['review_count'].mean()\n",
    "plt.axvline(median, color='k', linestyle='dashed', linewidth=1, label='Median Review Count')\n",
    "plt.axvline(mean, color='k', linestyle='solid', linewidth=1, label='Mean Review Count')\n",
    "plt.xlabel('Review Count')\n",
    "plt.ylabel('Log Frequency')\n",
    "plt.title('Log Distribution of Review Counts')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.25)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "# distribution of ratings\n",
    "rating_counts = rest_final.groupby(['rating'])['rating'].count()\n",
    "rating_counts.plot(kind='barh', color='m')\n",
    "plt.title('Number of Restaurants by Rating Category')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Ratings')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.25)\n",
    "plt.show()\n",
    "\n",
    "# count of each neighborhood\n",
    "custom_palette = sns.color_palette('Paired')\n",
    "rating_counts = rest_final.groupby(['neighborhood'])['neighborhood'].count().sort_values(ascending=True)\n",
    "rating_counts.plot(kind='barh', color=custom_palette)\n",
    "plt.title('Number of Restaurants by Neighborhood')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Neighborhood')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.25)\n",
    "plt.show()\n",
    "\n",
    "# counts of prices\n",
    "custom_labels = ['No Price', '\\$', '\\$\\$', '\\$\\$\\$', '\\$\\$\\$\\$']\n",
    "price_counts = rest_final.groupby(['price'])['price'].count()\n",
    "price_counts.plot(kind='barh', color='g')\n",
    "plt.title('Number of Restaurants by Price Range')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Prices')\n",
    "plt.yticks(range(len(custom_labels)), custom_labels)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.25)\n",
    "plt.show()\n",
    "\n",
    "# counts of tags\n",
    "tag_counts = rest_final.loc[:, ['delivery', 'pickup', 'restaurant_reservation', 'has_image', 'has_phone', 'has_st_add']].sum().sort_values(ascending=True)\n",
    "custom_palette = sns.color_palette('husl')\n",
    "tag_counts.plot(kind='barh', color=custom_palette)\n",
    "plt.axvline(x=rest_final.shape[0], color='k', linestyle='dashed', label=f'Total # of Restaurants') \n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Number of Restaurants by Tag Presence')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Tags')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.25)\n",
    "plt.show()\n",
    "\n",
    "# cuisine occurences\n",
    "cuisine_counts = rest_final.loc[:, 'acaibowls':'wraps'].sum().sort_values(ascending = True)\n",
    "custom_palette = sns.color_palette('Paired')\n",
    "cuisine_counts[-20:].plot(kind='barh', color=custom_palette)\n",
    "plt.title('Top 20 Cuisines in Boston')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Cuisines')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.25)\n",
    "plt.show()\n",
    "\n",
    "# median brs by neighborhood\n",
    "neighborhoods = rest_final.groupby(['neighborhood'])['brs'].median().sort_values(ascending=True)\n",
    "custom_palette = sns.color_palette('Paired')\n",
    "neighborhoods.plot(kind='barh', color=custom_palette, label='')\n",
    "plt.axvline(x=rest_final['brs'].median(), color='k', linestyle='dashed', label='Overall BRS Median')\n",
    "plt.title('Median BRS by Neighborhood')\n",
    "plt.xlabel('BRS')\n",
    "plt.ylabel('Neighborhood')\n",
    "plt.legend()\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.25)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# splitting data into X (features) and y (target)\n",
    "y = rest_final['brs']\n",
    "neighborhoods = rest_final['neighborhood'].unique().tolist()\n",
    "X = rest_final.drop((neighborhoods + ['id', 'name', 'review_count', 'rating', 'neighborhood', 'latitude', 'longitude', 'brs']), axis=1)\n",
    "\n",
    "# splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]\n",
    "# number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 25]\n",
    "# minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4, 10]\n",
    "# method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# randomized search on random forest hyperparams\n",
    "rs_forest = RandomForestRegressor()\n",
    "forest_random = RandomizedSearchCV(estimator = rs_forest, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "forest_random.fit(X_train, y_train)\n",
    "y_pred = forest_random.predict(X_test)\n",
    "\n",
    "# calculate mean squared error and r2\n",
    "rs_mse = mean_squared_error(y_test, y_pred)\n",
    "rs_r2 = r2_score(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {rs_mse}')\n",
    "print(f'R Squared Score is: {rs_r2}')\n",
    "print(forest_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [False],\n",
    "    'max_depth': [74, 77, 80],\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_leaf': [2, 3, 4],\n",
    "    'min_samples_split': [6, 7, 8, 9],\n",
    "    'n_estimators': [100, 104, 105, 106, 107, 108, 109]\n",
    "}\n",
    "\n",
    "gs_forest = RandomForestRegressor()\n",
    "# instantiate the grid search model\n",
    "rf_grid_search = GridSearchCV(estimator = gs_forest, param_grid = param_grid,\n",
    "                              scoring='neg_mean_squared_error', cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "# fit the grid search\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "y_pred = rf_grid_search.predict(X_test)\n",
    "rf_best_params = rf_grid_search.best_params_\n",
    "rf_best_estimator = rf_grid_search.best_estimator_\n",
    "\n",
    "# calculate mean squared error and r2\n",
    "rf_gs_mse = mean_squared_error(y_test, y_pred)\n",
    "rf_gs_r2 = r2_score(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {rf_gs_mse}')\n",
    "print(f'R Squared Score is: {rf_gs_r2}')\n",
    "print('MSE Improvement of {:0.5f}%.'.format(100 * (rf_gs_mse - rs_mse) / rs_mse))\n",
    "print('R2 Improvement of {:0.5f}%.'.format(100 * (rf_gs_r2 - rs_r2) / rs_r2))\n",
    "\n",
    "# feature importance\n",
    "gs_rf_importances = pd.DataFrame(rf_grid_search.best_estimator_.feature_importances_, index=X_train.columns, columns=['Importance'])\n",
    "gs_rf_importances.sort_values(by='Importance', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# baseline xgb regressor\n",
    "baseline_xgb = XGBRegressor()\n",
    "baseline_xgb.fit(X_train, y_train)\n",
    "y_pred = baseline_xgb.predict(X_test)\n",
    "\n",
    "# calculate mean squared error and r2\n",
    "baseline_xgb_mse = mean_squared_error(y_test, y_pred)\n",
    "baseline_xgb_r2 = r2_score(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {baseline_xgb_mse}')\n",
    "print(f'R Squared Score is: {baseline_xgb_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search cv \n",
    "param_grid = {\n",
    "    'n_estimators': [95, 100, 105],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.12, 0.11, 0.13]\n",
    "}\n",
    "\n",
    "# create the GridSearchCV instance\n",
    "xgb_grid_search = GridSearchCV(estimator=XGBRegressor(),\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='neg_mean_squared_error',  # Use negative mean squared error for regression\n",
    "                           cv=3) \n",
    "\n",
    "# fit the grid search \n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "y_pred = xgb_grid_search.predict(X_test)\n",
    "xgb_best_params = xgb_grid_search.best_params_\n",
    "xgb_best_estimator = xgb_grid_search.best_estimator_\n",
    "\n",
    "# calculate mean squared error and r2\n",
    "xgb_gs_mse = mean_squared_error(y_test, y_pred)\n",
    "xgb_gs_r2 = r2_score(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {xgb_gs_mse}')\n",
    "print(f'R Squared Score is: {xgb_gs_r2}')\n",
    "print('MSE Improvement of {:0.5f}%.'.format(100 * (xgb_gs_mse - baseline_xgb_mse) / baseline_xgb_mse))\n",
    "print('R2 Improvement of {:0.5f}%.'.format(100 * (xgb_gs_r2 - baseline_xgb_r2) / baseline_xgb_r2))\n",
    "\n",
    "# feature importance\n",
    "gs_xgb_importances = pd.DataFrame(xgb_best_estimator.feature_importances_, index=X_train.columns, columns=['Importance'])\n",
    "gs_xgb_importances.sort_values(by='Importance', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare the metrics and feature importances of the two tuned models\n",
    "print('MSE Difference of {:0.5f}%.'.format(xgb_gs_mse - rf_gs_mse))\n",
    "print('R2 Difference of {:0.5f}%.'.format(xgb_gs_r2 - rf_gs_r2))\n",
    "\n",
    "# feature selection\n",
    "X_rf = X[gs_rf_importances.index[:20].tolist()]\n",
    "X_xgb = X[gs_xgb_importances.index[:20].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retune after feature selection\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_rf, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [20, 50, 80],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [2, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [50, 100, 150]\n",
    "}\n",
    "\n",
    "# create a based model\n",
    "gs_forest = RandomForestRegressor()\n",
    "# instantiate the grid search model\n",
    "rf_grid_search = GridSearchCV(estimator = gs_forest, param_grid = param_grid,\n",
    "                            scoring='neg_mean_squared_error', cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "# fit the grid search\n",
    "rf_grid_search.fit(X_train_rf, y_train_rf)\n",
    "y_pred_rf = rf_grid_search.predict(X_test_rf)\n",
    "rf_best_params = rf_grid_search.best_params_\n",
    "rf_best_estimator = rf_grid_search.best_estimator_\n",
    "\n",
    "# calculate mean squared error and r2\n",
    "rf_gs_mse = mean_squared_error(y_test_rf, y_pred_rf)\n",
    "rf_gs_r2 = r2_score(y_test_rf, y_pred_rf)\n",
    "\n",
    "# feature importance\n",
    "gs_rf_importances = pd.DataFrame(rf_grid_search.best_estimator_.feature_importances_, index=X_train_rf.columns, columns=['Importance'])\n",
    "gs_rf_importances.sort_values(by='Importance', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_xgb, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# grid search cv \n",
    "param_grid = {\n",
    "    'n_estimators': [50, 75, 100, 110],\n",
    "    'max_depth': [2, 3, 4, 6, 8, 12],\n",
    "    'learning_rate': [0.25, 0.2, 0.15, 0.12, 0.11, 0.13]\n",
    "}\n",
    "\n",
    "# create the GridSearchCV instance\n",
    "xgb_grid_search = GridSearchCV(estimator=XGBRegressor(),\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='neg_mean_squared_error',  # Use negative mean squared error for regression\n",
    "                           cv=3) \n",
    "\n",
    "# fit the grid search \n",
    "xgb_grid_search.fit(X_train_xgb, y_train_xgb)\n",
    "xgb_best_estimator = xgb_grid_search.best_estimator_\n",
    "y_pred_xgb = xgb_best_estimator.predict(X_test_xgb)\n",
    "xgb_best_params = xgb_grid_search.best_params_\n",
    "\n",
    "# calculate mean squared error and r2\n",
    "xgb_gs_mse = mean_squared_error(y_test_xgb, y_pred_xgb)\n",
    "xgb_gs_r2 = r2_score(y_test_xgb, y_pred_xgb)\n",
    "\n",
    "\n",
    "# feature importance\n",
    "gs_xgb_importances = pd.DataFrame(xgb_best_estimator.feature_importances_, index=X_train_xgb.columns, columns=['Importance'])\n",
    "gs_xgb_importances.sort_values(by='Importance', ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the metrics and feature importances of the two tuned models\n",
    "print(f'Random Forest Mean Squared Error: {rf_gs_mse}')\n",
    "print(f'Random Forest R Squared Score is: {rf_gs_r2}')\n",
    "\n",
    "print(f'XGBoost Mean Squared Error: {xgb_gs_mse}')\n",
    "print(f'XGBoost R Squared Score is: {xgb_gs_r2}')\n",
    "\n",
    "print('MSE Difference (XGBoost - Random Forest) of {:0.5f}%.'.format(xgb_gs_mse - rf_gs_mse))\n",
    "print('R2 Difference (XGBoost - Random Forest) of {:0.5f}%.'.format(xgb_gs_r2 - rf_gs_r2))\n",
    "\n",
    "# random forest performs a little better\n",
    "plt.scatter(y_test_rf, y_pred_rf, c='red', alpha=0.7, label='Restaurants')\n",
    "fit = np.polyfit(y_test_rf, y_pred_rf, 1)\n",
    "plt.plot(y_test_rf, fit[0] * y_test_rf + fit[1], color='k', linestyle='solid', label='Trendline')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Scatter Plot of Actual vs. Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.25)\n",
    "plt.show()\n",
    "\n",
    "residuals = [y_test - y_pred for y_test, y_pred in zip(y_test_rf, y_pred_rf)]\n",
    "sns.displot(residuals, kde=True, color='red', bins=15, edgecolor='black')\n",
    "plt.xlabel('Residuals (Actual - Predicted)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Residuals')\n",
    "mean_residual = round(np.mean(residuals), 5)\n",
    "median_residual = round(np.median(residuals), 5)\n",
    "std_dev = round(np.std(residuals), 5)\n",
    "plt.text(0.45, 0.1, f'Mean: {mean_residual}\\nMedian: {median_residual}\\nStd Dev: {std_dev}', transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.grid(True, linestyle='--', alpha=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest feature importance\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.barplot(x=gs_rf_importances['Importance'], y=gs_rf_importances.index)\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature Names')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.25)\n",
    "plt.show()\n",
    "\n",
    "# xgboost feature importance\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.barplot(x=gs_xgb_importances['Importance'], y=gs_xgb_importances.index)\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature Names')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the brs by important features\n",
    "custom_labels = ['No Price', '\\$', '\\$\\$', '\\$\\$\\$', '\\$\\$\\$\\$']\n",
    "colors = sns.color_palette('Set2')\n",
    "sns.boxplot(x=X_rf['price'], y=y, palette=colors)\n",
    "plt.title('Distribution of Balanced Rating Score by Price Range')\n",
    "plt.xlabel('Price Range')\n",
    "plt.ylabel('BRS')\n",
    "plt.xticks(range(len(custom_labels)), custom_labels)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.25)\n",
    "plt.show()\n",
    "\n",
    "feat_graph_list = list(X_rf.columns.values)\n",
    "feat_graph_list.remove('price')\n",
    "for feature in feat_graph_list:\n",
    "    custom_labels = ['No', 'Yes']\n",
    "    colors = sns.color_palette('Set2')\n",
    "    sns.boxplot(x = X[feature], y = y, palette=colors)\n",
    "    plt.title(f'Distribution of Balanced Rating Score by {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('BRS')\n",
    "    plt.xticks(range(len(custom_labels)), custom_labels)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.25)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (v3.9.12:b28265d7e6, Mar 23 2022, 18:17:11) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
